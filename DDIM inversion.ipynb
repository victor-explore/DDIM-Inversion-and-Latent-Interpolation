{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a DDIM inversion method. Get the (inverted) latents for a pair of real images and plot the generated images obtained via linear\n",
    "interpolation of the latents corresponding to these image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch  # Import PyTorch library\n",
    "import torch.nn as nn  # Import neural network module\n",
    "import torch.optim as optim  # Import optimization module\n",
    "from torchvision import datasets, transforms  # Import datasets and transforms\n",
    "from torchvision.utils import save_image, make_grid  # Import utility to save images\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision  # Import torchvision library\n",
    "import matplotlib.pyplot as plt  # Import plotting library\n",
    "import os  # Import os module for file operations\n",
    "import numpy as np  # Import numpy library        nn.InstanceNorm2d(out_channels),\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image  # Import PIL for image processing\n",
    "import scipy\n",
    "from torchvision.models import resnet50\n",
    "from tqdm.notebook import trange\n",
    "from pathlib import Path\n",
    "import math\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 600\n",
    "IMG_SIZE = 128\n",
    "TRAIN_DDIM = True\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "STEP_GAMMA = 0.3\n",
    "SCALING_FACTOR = 5000\n",
    "BUTTERFLY = \"butterfly\"\n",
    "ANIMAL = \"animal\"\n",
    "dataset = os.getenv('DATASET', BUTTERFLY)  # butterfly or animal\n",
    "\n",
    "NOISE_STEPS = 450\n",
    "SAMPLING_STEPS = NOISE_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "LOG_DIR = log_dir = f'DDIM/tensorboard/{dataset}'\n",
    "IMG_DIR = f'DDIM/generated_images/{dataset}'\n",
    "\n",
    "\n",
    "def recreate_directory(dir_path):\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(dir_path):\n",
    "        # Delete the directory if it exists\n",
    "        shutil.rmtree(dir_path)\n",
    "    # Create the directory\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "os.makedirs('DDIM/models', exist_ok=True)\n",
    "\n",
    "if TRAIN_DDIM:\n",
    "    recreate_directory(LOG_DIR)\n",
    "    recreate_directory(IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tensorboard stuff\n",
    "writer = SummaryWriter(LOG_DIR)\n",
    "\n",
    "\n",
    "def log_losses_to_tensorboard(epoch, loss):\n",
    "    writer.add_scalar('Loss/Total', loss, epoch)\n",
    "\n",
    "\n",
    "def log_gradients_to_tensorboard(model, epoch):\n",
    "    total_norm = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            norm = param.grad.norm(2).item()\n",
    "            total_norm += norm ** 2\n",
    "            writer.add_scalar(f'Gradients/{name}', norm, epoch)\n",
    "    total_norm = total_norm ** 0.5\n",
    "    writer.add_scalar(f'Gradients/total_norm', total_norm, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Directory containing all images (including subfolders)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform  # Transformations to apply to images\n",
    "        self.image_files = []  # List to store all image file paths\n",
    "\n",
    "        # Traverse through all subfolders\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_files.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)  # Return the total number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]  # Get image path\n",
    "        image = Image.open(img_path).convert(\n",
    "            'RGB')  # Open image and convert to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if any\n",
    "\n",
    "        return image, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # Randomly flip images horizontally\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    # Convert images to PyTorch tensors and scale to [0, 1]\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "butterly_dataset = ImageDataset(root_dir=\"data/butterfly_data\", transform=transform)\n",
    "\n",
    "animal_dataset = datasets.ImageFolder(root='data/Animals_data/animals/animals',  # Specify the root directory of the dataset\n",
    "                               transform=transform)  # Apply the defined transformations to the dataset\n",
    "\n",
    "ds = butterly_dataset if dataset == BUTTERFLY else animal_dataset\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(ds, BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, time_step):\n",
    "        half_dim = self.embed_dim // 2\n",
    "        freqs = torch.exp(-math.log(SCALING_FACTOR) *\n",
    "                          torch.arange(half_dim, device=time_step.device) / half_dim)\n",
    "        angles = time_step[:, None] * freqs[None, :]  # Shape: [B, half_dim]\n",
    "        # Shape: [B, embed_dim]\n",
    "        pos_embed = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        return pos_embed\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_embed_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Time embedding projection to match out_channels\n",
    "        self.time_dense = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual connection adjustment if input and output channels differ\n",
    "        self.residual_conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_embed):\n",
    "        residual = self.residual_conv(x)\n",
    "\n",
    "        # First convolution + norm + time embedding addition\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Add time embedding\n",
    "        # Shape: [B, out_channels, 1, 1]\n",
    "        time_embed = self.time_dense(time_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x + time_embed\n",
    "\n",
    "        # Activation\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Second convolution + norm\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Add residual and apply activation\n",
    "        return self.relu(x + residual)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = in_channels // num_heads\n",
    "        assert in_channels % num_heads == 0, \"in_channels must be divisible by num_heads\"\n",
    "\n",
    "        self.query = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.out = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        # (B, num_heads, head_dim, H*W)\n",
    "        query = self.query(x).view(B, self.num_heads, self.head_dim, -1)\n",
    "        # (B, num_heads, head_dim, H*W)\n",
    "        key = self.key(x).view(B, self.num_heads, self.head_dim, -1)\n",
    "        # (B, num_heads, head_dim, H*W)\n",
    "        value = self.value(x).view(B, self.num_heads, self.head_dim, -1)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # (B, num_heads, H*W, H*W)\n",
    "        attn_scores = torch.einsum('bhqd,bhkd->bhqk', query, key) * self.scale\n",
    "        attn = self.softmax(attn_scores)\n",
    "\n",
    "        # Apply attention to values\n",
    "        # (B, num_heads, head_dim, H*W)\n",
    "        out = torch.einsum('bhqk,bhvd->bhqd', attn, value)\n",
    "        out = out.contiguous().view(B, C, H, W)  # Reshape back to original dims\n",
    "        return self.out(out) + x  # Residual connection\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3, base_filters=64, embed_dim=32):\n",
    "        super(UNet, self).__init__()\n",
    "        self.time_embedding = SinusoidalPositionalEmbedding(embed_dim)\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = ResidualBlock(input_channels, base_filters, embed_dim)\n",
    "        self.enc2 = ResidualBlock(base_filters, base_filters * 2, embed_dim)\n",
    "        self.enc3 = ResidualBlock(\n",
    "            base_filters * 2, base_filters * 4, embed_dim)\n",
    "        self.enc4 = ResidualBlock(\n",
    "            base_filters * 4, base_filters * 8, embed_dim)\n",
    "\n",
    "        self.attn1 = MultiHeadAttention(base_filters * 8)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(\n",
    "            base_filters * 8, base_filters * 16, embed_dim)\n",
    "        self.attn2 = MultiHeadAttention(base_filters * 16)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = ResidualBlock(\n",
    "            base_filters * 16, base_filters * 8, embed_dim)\n",
    "        self.attn3 = MultiHeadAttention(base_filters * 8)\n",
    "\n",
    "        self.dec3 = ResidualBlock(\n",
    "            base_filters*4 + base_filters * 8, base_filters * 4, embed_dim)\n",
    "        self.dec2 = ResidualBlock(\n",
    "            base_filters * 2+base_filters * 4, base_filters * 2, embed_dim)\n",
    "        self.dec1 = ResidualBlock(\n",
    "            base_filters + base_filters * 2, base_filters, embed_dim)\n",
    "\n",
    "        # Final Output\n",
    "        self.final = nn.Conv2d(base_filters, output_channels, kernel_size=1)\n",
    "\n",
    "        # MaxPooling and Upsampling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            base_filters * 16, base_filters * 8, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, time_step):\n",
    "        # Generate time-step embedding\n",
    "        time_embed = self.time_embedding(time_step)  # Shape: [B, embed_dim]\n",
    "\n",
    "        # Encoder path\n",
    "        enc1 = self.enc1(x, time_embed)\n",
    "        enc2 = self.enc2(self.pool(enc1), time_embed)\n",
    "        enc3 = self.enc3(self.pool(enc2), time_embed)\n",
    "        enc4 = self.attn1(self.enc4(self.pool(enc3), time_embed))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.attn2(self.bottleneck(self.pool(enc4), time_embed))\n",
    "\n",
    "        # Decoder path\n",
    "        dec4 = self.attn3(self.dec4(\n",
    "            torch.cat([self.upsample(bottleneck), enc4], dim=1), time_embed))\n",
    "        dec3 = self.dec3(\n",
    "            torch.cat([F.interpolate(dec4, scale_factor=2), enc3], dim=1), time_embed)\n",
    "        dec2 = self.dec2(\n",
    "            torch.cat([F.interpolate(dec3, scale_factor=2), enc2], dim=1), time_embed)\n",
    "        dec1 = self.dec1(\n",
    "            torch.cat([F.interpolate(dec2, scale_factor=2), enc1], dim=1), time_embed)\n",
    "\n",
    "        return self.final(dec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(DataLoader(ds, batch_size=1)))\n",
    "t = torch.tensor([1])\n",
    "summary(UNet(), input_data=[images, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "betas = torch.linspace(1e-4, 0.02, NOISE_STEPS).to(device)\n",
    "alphas = 1 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def forward_diffusion(x0: torch.Tensor, t: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    noise = torch.randn_like(x0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_alphas_cumprod, t, x0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x0.shape)\n",
    "    xt = sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    return xt, noise\n",
    "\n",
    "\n",
    "def ddim_sample(\n",
    "    model: UNet,\n",
    "    x_T: torch.Tensor,\n",
    "    sampling_steps: int,\n",
    "):\n",
    "    model.eval()\n",
    "    # Get sampling timesteps (you can choose these differently)\n",
    "    steps = torch.linspace(0, NOISE_STEPS - 1,\n",
    "                           sampling_steps).flip(0).int().to(device)\n",
    "\n",
    "    x_t = x_T\n",
    "    for i, step in enumerate(steps):\n",
    "        if i == len(steps) - 1:\n",
    "            step_prev = 0\n",
    "        else:\n",
    "            step_prev = steps[i + 1]\n",
    "\n",
    "        alpha_cumprod_t = alphas_cumprod[step]\n",
    "        alpha_cumprod_t_prev = alphas_cumprod[step_prev]\n",
    "\n",
    "        # Predict noise\n",
    "        t_batch = step.repeat(x_t.shape[0])\n",
    "        with torch.no_grad():\n",
    "            predicted_noise = model(x_t, t_batch)\n",
    "\n",
    "        # Get the predicted x0\n",
    "        sqrt_alpha_cumprod_t = torch.sqrt(alpha_cumprod_t)\n",
    "        sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alpha_cumprod_t)\n",
    "        predicted_x0 = (x_t - sqrt_one_minus_alpha_cumprod_t *\n",
    "                        predicted_noise) / sqrt_alpha_cumprod_t\n",
    "\n",
    "        # DDIM formula\n",
    "        sqrt_alpha_cumprod_t_prev = torch.sqrt(alpha_cumprod_t_prev)\n",
    "        sqrt_one_minus_alpha_cumprod_t_prev = torch.sqrt(\n",
    "            1 - alpha_cumprod_t_prev)\n",
    "\n",
    "        x_t = sqrt_alpha_cumprod_t_prev * predicted_x0 + \\\n",
    "            sqrt_one_minus_alpha_cumprod_t_prev * predicted_noise\n",
    "\n",
    "    return torch.clamp(x_t, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model: UNet, epoch_index, img_dir, display=False):\n",
    "    n_rows = 5\n",
    "    # Generate and save images for the current epoch\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        model.eval()\n",
    "        # Sample 4 images\n",
    "        x_T = torch.randn(n_rows*n_rows, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "        samples = ddim_sample(model, x_T, sampling_steps=SAMPLING_STEPS)\n",
    "\n",
    "    if display:\n",
    "        # Function to display grid using matplotlib\n",
    "        def show_images(tensor, title):\n",
    "            tensor = (tensor + 1) / 2.0\n",
    "            grid = make_grid(tensor, nrow=n_rows, padding=2,\n",
    "                             normalize=True)  # Create grid\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            # Convert to numpy for plotting\n",
    "            plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "            plt.axis('off')\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "\n",
    "        show_images(samples, f'Generated Images - Epoch {epoch_index + 1}')\n",
    "    else:\n",
    "\n",
    "        # Save generated images\n",
    "        image_path_gen = f'{img_dir}/generated_{epoch_index + 1}.png'\n",
    "        save_image(make_grid(samples, nrow=n_rows, padding=2,\n",
    "                   normalize=True), image_path_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_ddim(\n",
    "    num_epochs: int\n",
    "):\n",
    "    model = torch.nn.DataParallel(UNet().to(device))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=num_epochs//4, gamma=STEP_GAMMA)\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        model.train()\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Sample t uniformly\n",
    "            t = torch.randint(0, NOISE_STEPS, (images.shape[0],)).to(device)\n",
    "\n",
    "            # Get noisy image and noise\n",
    "            x_t, noise = forward_diffusion(images, t)\n",
    "\n",
    "            # Predict noise\n",
    "            predicted_noise = model(x_t, t)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        log_gradients_to_tensorboard(model, epoch)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}]  loss: {loss.item():.4f}\")\n",
    "            generate_and_save_images(model, epoch, img_dir=IMG_DIR)\n",
    "\n",
    "        log_losses_to_tensorboard(\n",
    "            epoch, loss.item())\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "ddim_model_path = f'DDIM/models/model_{dataset}.pth'\n",
    "\n",
    "if TRAIN_DDIM:\n",
    "    model = train_ddim(num_epochs=NUM_EPOCHS)\n",
    "    torch.save(model.state_dict(), ddim_model_path)\n",
    "    print(f\"Models saved in {ddim_model_path}\")\n",
    "else:\n",
    "    model = torch.nn.DataParallel(UNet().to(device))\n",
    "    model.load_state_dict(torch.load(ddim_model_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generate_and_save_images(model, NUM_EPOCHS, img_dir=IMG_DIR, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDIM Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ddim_inversion(\n",
    "    unet: nn.Module,\n",
    "    x0: torch.Tensor,\n",
    "    num_steps: int,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    # Initialize x_t with x0\n",
    "    x_t = x0\n",
    "\n",
    "    # DDIM inversion steps\n",
    "    for t in range(num_steps):\n",
    "        t_tensor = torch.full(\n",
    "            (x0.shape[0],), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict noise\n",
    "        with torch.no_grad():\n",
    "            predicted_noise = unet(x_t, t_tensor)\n",
    "\n",
    "        # If not the final step\n",
    "        if t < num_steps - 1:\n",
    "            # Current alpha and sigma\n",
    "            alpha_t = sqrt_alphas_cumprod[t]\n",
    "            sigma_t = sqrt_one_minus_alphas_cumprod[t]\n",
    "\n",
    "            # Next timestep's alpha and sigma\n",
    "            alpha_next = sqrt_alphas_cumprod[t + 1]\n",
    "            sigma_next = sqrt_one_minus_alphas_cumprod[t + 1]\n",
    "\n",
    "            # Predict x0\n",
    "            pred_x0 = (x_t - sigma_t * predicted_noise) / alpha_t\n",
    "\n",
    "            # DDIM update step\n",
    "            x_t = alpha_next * pred_x0 + sigma_next * predicted_noise\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def sample_images(ds, num_samples=2):\n",
    "    loader = DataLoader(ds, batch_size=num_samples, shuffle=True)\n",
    "    real_images, _ = next(iter(loader))\n",
    "    return real_images.to(device)\n",
    "\n",
    "# Function to linearly interpolate latents\n",
    "def interpolate_latents(latent1, latent2, num_steps=10):\n",
    "    interpolations = torch.stack([latent1 * (1 - alpha) + latent2 * alpha for alpha in np.linspace(0, 1, num_steps)])\n",
    "    return interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "real_images = sample_images(ds)\n",
    "latents = ddim_inversion(model, real_images, num_steps=NOISE_STEPS)\n",
    "latent1, latent2 = latents[0], latents[1]\n",
    "latents = interpolate_latents(latent1, latent2)\n",
    "generated_images = ddim_sample(model, latents, sampling_steps=SAMPLING_STEPS)\n",
    "\n",
    "\n",
    "def plot_images(real_images, generated_images):\n",
    "    # Concatenate real and interpolated images for plotting\n",
    "    images_to_plot = torch.cat(\n",
    "        [real_images[0:1], generated_images, real_images[1:2]], dim=0)\n",
    "\n",
    "    images = images_to_plot.cpu().permute(0, 2, 3, 1).numpy()  # Convert to HWC\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(15, 2))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        axes[i].imshow((img + 1) * 0.5)  # Assuming normalization [-1, 1]\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle(\"DDIM Inversion Interpolation\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call plot_images\n",
    "plot_images(real_images, generated_images)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
